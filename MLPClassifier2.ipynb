{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FA6ib67JO0y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score,f1_score,roc_auc_score,roc_curve\n",
        "from sklearn.model_selection import cross_val_score,KFold,cross_val_predict,StratifiedKFold,GridSearchCV\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVYySJ_hKyVr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def handleMIssingVals(df2):\n",
        "  df2['ps_ind_02_cat'] = df2['ps_ind_02_cat'].fillna(1.0)\n",
        "  df2['ps_ind_04_cat']=df2['ps_ind_04_cat'].fillna(0.0)\n",
        "  df2['ps_ind_05_cat']=df2['ps_ind_05_cat'].fillna(7.0)\n",
        "  df2['ps_reg_03']=df2['ps_reg_03'].fillna(df2['ps_reg_03'].mean())\n",
        "  df2['ps_car_01_cat']=df2['ps_car_01_cat'].fillna(11.0)\n",
        "  df2['ps_car_05_cat']=df2['ps_car_05_cat'].fillna(2.0)\n",
        "  df2['ps_car_07_cat']=df2['ps_car_07_cat'].fillna(1.0)\n",
        "  df2['ps_car_09_cat']=df2['ps_car_09_cat'].fillna(2.0)\n",
        "  df2['ps_car_11']=df2['ps_car_11'].fillna(3.0)\n",
        "  df2['ps_car_12']=df2['ps_car_12'].fillna(df2['ps_car_12'].mean())\n",
        "  df2['ps_car_14']=df2['ps_car_14'].fillna(df2['ps_car_14'].mean())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJZIo4tFKqUZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df=pd.read_csv('/content/drive/My Drive/Colab Notebooks/auto.csv')\n",
        "df=df.replace(-1,np.nan)\n",
        "handleMIssingVals(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIs1BV2xLxA5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "drop_cols=['id', 'ps_car_13' ,'ps_car_14' , 'ps_reg_03' , 'ps_ind_10_bin' , 'ps_ind_11_bin' , 'ps_ind_13_bin' ,'ps_ind_14' , 'ps_car_03_cat','target']\n",
        "df.drop(drop_cols,axis=1,inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KH0O1ahcLMLZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(df,target, test_size= 0.2, random_state=27)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvZPaL_3MmeN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf = MLPClassifier(hidden_layer_sizes=(100,100,100), max_iter=1000, alpha=0.0001,\n",
        "                     solver='sgd', verbose=10,  random_state=21,tol=0.000000001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oP7EsZd2MnGc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9cec00d4-2b32-4bf2-8548-c716df0dfb7c"
      },
      "source": [
        "clf.fit(x_train,y_train)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.16504640\n",
            "Iteration 2, loss = 0.15657484\n",
            "Iteration 3, loss = 0.15599840\n",
            "Iteration 4, loss = 0.15572189\n",
            "Iteration 5, loss = 0.15545164\n",
            "Iteration 6, loss = 0.15536726\n",
            "Iteration 7, loss = 0.15518174\n",
            "Iteration 8, loss = 0.15493828\n",
            "Iteration 9, loss = 0.15485565\n",
            "Iteration 10, loss = 0.15478602\n",
            "Iteration 11, loss = 0.15465210\n",
            "Iteration 12, loss = 0.15461967\n",
            "Iteration 13, loss = 0.15452151\n",
            "Iteration 14, loss = 0.15443361\n",
            "Iteration 15, loss = 0.15434555\n",
            "Iteration 16, loss = 0.15430441\n",
            "Iteration 17, loss = 0.15417642\n",
            "Iteration 18, loss = 0.15424073\n",
            "Iteration 19, loss = 0.15418434\n",
            "Iteration 20, loss = 0.15412441\n",
            "Iteration 21, loss = 0.15401922\n",
            "Iteration 22, loss = 0.15403687\n",
            "Iteration 23, loss = 0.15397536\n",
            "Iteration 24, loss = 0.15390153\n",
            "Iteration 25, loss = 0.15389963\n",
            "Iteration 26, loss = 0.15386404\n",
            "Iteration 27, loss = 0.15383446\n",
            "Iteration 28, loss = 0.15377199\n",
            "Iteration 29, loss = 0.15377642\n",
            "Iteration 30, loss = 0.15372966\n",
            "Iteration 31, loss = 0.15376469\n",
            "Iteration 32, loss = 0.15365958\n",
            "Iteration 33, loss = 0.15363109\n",
            "Iteration 34, loss = 0.15357105\n",
            "Iteration 35, loss = 0.15358632\n",
            "Iteration 36, loss = 0.15353921\n",
            "Iteration 37, loss = 0.15349767\n",
            "Iteration 38, loss = 0.15346594\n",
            "Iteration 39, loss = 0.15345430\n",
            "Iteration 40, loss = 0.15346746\n",
            "Iteration 41, loss = 0.15338389\n",
            "Iteration 42, loss = 0.15341842\n",
            "Iteration 43, loss = 0.15338247\n",
            "Iteration 44, loss = 0.15335021\n",
            "Iteration 45, loss = 0.15332760\n",
            "Iteration 46, loss = 0.15332670\n",
            "Iteration 47, loss = 0.15325573\n",
            "Iteration 48, loss = 0.15329696\n",
            "Iteration 49, loss = 0.15327909\n",
            "Iteration 50, loss = 0.15323887\n",
            "Iteration 51, loss = 0.15318223\n",
            "Iteration 52, loss = 0.15317247\n",
            "Iteration 53, loss = 0.15309374\n",
            "Iteration 54, loss = 0.15313448\n",
            "Iteration 55, loss = 0.15308964\n",
            "Iteration 56, loss = 0.15309468\n",
            "Iteration 57, loss = 0.15304260\n",
            "Iteration 58, loss = 0.15306310\n",
            "Iteration 59, loss = 0.15306854\n",
            "Iteration 60, loss = 0.15303248\n",
            "Iteration 61, loss = 0.15299505\n",
            "Iteration 62, loss = 0.15294245\n",
            "Iteration 63, loss = 0.15298414\n",
            "Iteration 64, loss = 0.15295859\n",
            "Iteration 65, loss = 0.15297440\n",
            "Iteration 66, loss = 0.15290483\n",
            "Iteration 67, loss = 0.15285750\n",
            "Iteration 68, loss = 0.15281762\n",
            "Iteration 69, loss = 0.15283006\n",
            "Iteration 70, loss = 0.15283492\n",
            "Iteration 71, loss = 0.15286512\n",
            "Iteration 72, loss = 0.15274841\n",
            "Iteration 73, loss = 0.15274788\n",
            "Iteration 74, loss = 0.15276738\n",
            "Iteration 75, loss = 0.15273457\n",
            "Iteration 76, loss = 0.15272055\n",
            "Iteration 77, loss = 0.15268790\n",
            "Iteration 78, loss = 0.15269079\n",
            "Iteration 79, loss = 0.15264768\n",
            "Iteration 80, loss = 0.15265717\n",
            "Iteration 81, loss = 0.15265681\n",
            "Iteration 82, loss = 0.15261990\n",
            "Iteration 83, loss = 0.15255746\n",
            "Iteration 84, loss = 0.15255963\n",
            "Iteration 85, loss = 0.15256055\n",
            "Iteration 86, loss = 0.15256502\n",
            "Iteration 87, loss = 0.15255265\n",
            "Iteration 88, loss = 0.15249675\n",
            "Iteration 89, loss = 0.15247272\n",
            "Iteration 90, loss = 0.15251067\n",
            "Iteration 91, loss = 0.15245636\n",
            "Iteration 92, loss = 0.15243457\n",
            "Iteration 93, loss = 0.15247490\n",
            "Iteration 94, loss = 0.15242048\n",
            "Iteration 95, loss = 0.15245772\n",
            "Iteration 96, loss = 0.15238641\n",
            "Iteration 97, loss = 0.15235219\n",
            "Iteration 98, loss = 0.15236869\n",
            "Iteration 99, loss = 0.15232754\n",
            "Iteration 100, loss = 0.15231146\n",
            "Iteration 101, loss = 0.15231223\n",
            "Iteration 102, loss = 0.15232421\n",
            "Iteration 103, loss = 0.15227874\n",
            "Iteration 104, loss = 0.15227413\n",
            "Iteration 105, loss = 0.15221387\n",
            "Iteration 106, loss = 0.15221605\n",
            "Iteration 107, loss = 0.15222577\n",
            "Iteration 108, loss = 0.15220916\n",
            "Iteration 109, loss = 0.15219427\n",
            "Iteration 110, loss = 0.15218642\n",
            "Iteration 111, loss = 0.15214785\n",
            "Iteration 112, loss = 0.15218082\n",
            "Iteration 113, loss = 0.15214409\n",
            "Iteration 114, loss = 0.15212475\n",
            "Iteration 115, loss = 0.15216986\n",
            "Iteration 116, loss = 0.15208291\n",
            "Iteration 117, loss = 0.15212156\n",
            "Iteration 118, loss = 0.15206553\n",
            "Iteration 119, loss = 0.15204574\n",
            "Iteration 120, loss = 0.15206573\n",
            "Iteration 121, loss = 0.15203420\n",
            "Iteration 122, loss = 0.15200413\n",
            "Iteration 123, loss = 0.15195227\n",
            "Iteration 124, loss = 0.15206801\n",
            "Iteration 125, loss = 0.15198278\n",
            "Iteration 126, loss = 0.15199580\n",
            "Iteration 127, loss = 0.15195041\n",
            "Iteration 128, loss = 0.15187912\n",
            "Iteration 129, loss = 0.15192247\n",
            "Iteration 130, loss = 0.15197330\n",
            "Iteration 131, loss = 0.15186895\n",
            "Iteration 132, loss = 0.15190455\n",
            "Iteration 133, loss = 0.15187061\n",
            "Iteration 134, loss = 0.15186466\n",
            "Iteration 135, loss = 0.15190297\n",
            "Iteration 136, loss = 0.15182708\n",
            "Iteration 137, loss = 0.15181869\n",
            "Iteration 138, loss = 0.15182531\n",
            "Iteration 139, loss = 0.15177134\n",
            "Iteration 140, loss = 0.15181579\n",
            "Iteration 141, loss = 0.15173821\n",
            "Iteration 142, loss = 0.15175829\n",
            "Iteration 143, loss = 0.15170941\n",
            "Iteration 144, loss = 0.15167597\n",
            "Iteration 145, loss = 0.15177004\n",
            "Iteration 146, loss = 0.15168285\n",
            "Iteration 147, loss = 0.15172235\n",
            "Iteration 148, loss = 0.15173175\n",
            "Iteration 149, loss = 0.15165501\n",
            "Iteration 150, loss = 0.15161042\n",
            "Iteration 151, loss = 0.15161558\n",
            "Iteration 152, loss = 0.15156526\n",
            "Iteration 153, loss = 0.15163983\n",
            "Iteration 154, loss = 0.15153565\n",
            "Iteration 155, loss = 0.15162307\n",
            "Iteration 156, loss = 0.15156453\n",
            "Iteration 157, loss = 0.15154766\n",
            "Iteration 158, loss = 0.15155058\n",
            "Iteration 159, loss = 0.15152801\n",
            "Iteration 160, loss = 0.15151736\n",
            "Iteration 161, loss = 0.15153725\n",
            "Iteration 162, loss = 0.15149949\n",
            "Iteration 163, loss = 0.15150525\n",
            "Iteration 164, loss = 0.15140740\n",
            "Iteration 165, loss = 0.15152136\n",
            "Iteration 166, loss = 0.15140720\n",
            "Iteration 167, loss = 0.15144256\n",
            "Iteration 168, loss = 0.15142615\n",
            "Iteration 169, loss = 0.15143986\n",
            "Iteration 170, loss = 0.15134534\n",
            "Iteration 171, loss = 0.15135839\n",
            "Iteration 172, loss = 0.15134326\n",
            "Iteration 173, loss = 0.15134616\n",
            "Iteration 174, loss = 0.15139645\n",
            "Iteration 175, loss = 0.15129469\n",
            "Iteration 176, loss = 0.15130114\n",
            "Iteration 177, loss = 0.15130652\n",
            "Iteration 178, loss = 0.15127106\n",
            "Iteration 179, loss = 0.15125374\n",
            "Iteration 180, loss = 0.15120715\n",
            "Iteration 181, loss = 0.15126716\n",
            "Iteration 182, loss = 0.15125136\n",
            "Iteration 183, loss = 0.15115278\n",
            "Iteration 184, loss = 0.15126275\n",
            "Iteration 185, loss = 0.15121446\n",
            "Iteration 186, loss = 0.15115949\n",
            "Iteration 187, loss = 0.15121931\n",
            "Iteration 188, loss = 0.15120208\n",
            "Iteration 189, loss = 0.15114449\n",
            "Iteration 190, loss = 0.15108426\n",
            "Iteration 191, loss = 0.15112714\n",
            "Iteration 192, loss = 0.15112039\n",
            "Iteration 193, loss = 0.15104873\n",
            "Iteration 194, loss = 0.15106338\n",
            "Iteration 195, loss = 0.15099352\n",
            "Iteration 196, loss = 0.15107136\n",
            "Iteration 197, loss = 0.15099987\n",
            "Iteration 198, loss = 0.15104008\n",
            "Iteration 199, loss = 0.15097229\n",
            "Iteration 200, loss = 0.15098304\n",
            "Iteration 201, loss = 0.15097344\n",
            "Iteration 202, loss = 0.15094114\n",
            "Iteration 203, loss = 0.15095908\n",
            "Iteration 204, loss = 0.15093874\n",
            "Iteration 205, loss = 0.15091567\n",
            "Iteration 206, loss = 0.15092740\n",
            "Iteration 207, loss = 0.15090564\n",
            "Iteration 208, loss = 0.15085897\n",
            "Iteration 209, loss = 0.15084033\n",
            "Iteration 210, loss = 0.15082692\n",
            "Iteration 211, loss = 0.15084852\n",
            "Iteration 212, loss = 0.15081093\n",
            "Iteration 213, loss = 0.15076095\n",
            "Iteration 214, loss = 0.15076208\n",
            "Iteration 215, loss = 0.15078157\n",
            "Iteration 216, loss = 0.15072104\n",
            "Iteration 217, loss = 0.15067759\n",
            "Iteration 218, loss = 0.15076636\n",
            "Iteration 219, loss = 0.15068529\n",
            "Iteration 220, loss = 0.15073748\n",
            "Iteration 221, loss = 0.15071257\n",
            "Iteration 222, loss = 0.15067207\n",
            "Iteration 223, loss = 0.15063600\n",
            "Iteration 224, loss = 0.15065862\n",
            "Iteration 225, loss = 0.15060993\n",
            "Iteration 226, loss = 0.15067084\n",
            "Iteration 227, loss = 0.15061659\n",
            "Iteration 228, loss = 0.15062502\n",
            "Iteration 229, loss = 0.15053092\n",
            "Iteration 230, loss = 0.15054661\n",
            "Iteration 231, loss = 0.15052067\n",
            "Iteration 232, loss = 0.15051676\n",
            "Iteration 233, loss = 0.15049598\n",
            "Iteration 234, loss = 0.15048333\n",
            "Iteration 235, loss = 0.15043905\n",
            "Iteration 236, loss = 0.15044050\n",
            "Iteration 237, loss = 0.15048681\n",
            "Iteration 238, loss = 0.15041452\n",
            "Iteration 239, loss = 0.15040002\n",
            "Iteration 240, loss = 0.15043249\n",
            "Iteration 241, loss = 0.15036842\n",
            "Iteration 242, loss = 0.15034021\n",
            "Iteration 243, loss = 0.15030621\n",
            "Iteration 244, loss = 0.15034613\n",
            "Iteration 245, loss = 0.15031259\n",
            "Iteration 246, loss = 0.15028155\n",
            "Iteration 247, loss = 0.15027654\n",
            "Iteration 248, loss = 0.15029205\n",
            "Iteration 249, loss = 0.15023288\n",
            "Iteration 250, loss = 0.15026647\n",
            "Iteration 251, loss = 0.15029962\n",
            "Iteration 252, loss = 0.15018208\n",
            "Iteration 253, loss = 0.15016380\n",
            "Iteration 254, loss = 0.15016720\n",
            "Iteration 255, loss = 0.15018367\n",
            "Iteration 256, loss = 0.15009059\n",
            "Iteration 257, loss = 0.15019843\n",
            "Iteration 258, loss = 0.15004750\n",
            "Iteration 259, loss = 0.15006090\n",
            "Iteration 260, loss = 0.15010069\n",
            "Iteration 261, loss = 0.15008422\n",
            "Iteration 262, loss = 0.15005320\n",
            "Iteration 263, loss = 0.14999337\n",
            "Iteration 264, loss = 0.15010339\n",
            "Iteration 265, loss = 0.14996646\n",
            "Iteration 266, loss = 0.14999636\n",
            "Iteration 267, loss = 0.14991690\n",
            "Iteration 268, loss = 0.14995925\n",
            "Iteration 269, loss = 0.14994839\n",
            "Iteration 270, loss = 0.14995619\n",
            "Iteration 271, loss = 0.14988780\n",
            "Iteration 272, loss = 0.14986969\n",
            "Iteration 273, loss = 0.14973872\n",
            "Iteration 274, loss = 0.14981127\n",
            "Iteration 275, loss = 0.14984371\n",
            "Iteration 276, loss = 0.14976361\n",
            "Iteration 277, loss = 0.14982115\n",
            "Iteration 278, loss = 0.14980156\n",
            "Iteration 279, loss = 0.14977157\n",
            "Iteration 280, loss = 0.14974231\n",
            "Iteration 281, loss = 0.14977276\n",
            "Iteration 282, loss = 0.14960895\n",
            "Iteration 283, loss = 0.14968561\n",
            "Iteration 284, loss = 0.14963908\n",
            "Iteration 285, loss = 0.14971000\n",
            "Iteration 286, loss = 0.14963871\n",
            "Iteration 287, loss = 0.14960408\n",
            "Iteration 288, loss = 0.14961135\n",
            "Iteration 289, loss = 0.14958558\n",
            "Iteration 290, loss = 0.14954297\n",
            "Iteration 291, loss = 0.14958147\n",
            "Iteration 292, loss = 0.14956246\n",
            "Iteration 293, loss = 0.14950340\n",
            "Iteration 294, loss = 0.14953943\n",
            "Iteration 295, loss = 0.14951416\n",
            "Iteration 296, loss = 0.14952036\n",
            "Iteration 297, loss = 0.14949792\n",
            "Iteration 298, loss = 0.14941174\n",
            "Iteration 299, loss = 0.14943126\n",
            "Iteration 300, loss = 0.14942454\n",
            "Iteration 301, loss = 0.14929774\n",
            "Iteration 302, loss = 0.14936149\n",
            "Iteration 303, loss = 0.14928838\n",
            "Iteration 304, loss = 0.14929349\n",
            "Iteration 305, loss = 0.14927885\n",
            "Iteration 306, loss = 0.14920955\n",
            "Iteration 307, loss = 0.14927235\n",
            "Iteration 308, loss = 0.14932615\n",
            "Iteration 309, loss = 0.14925038\n",
            "Iteration 310, loss = 0.14919163\n",
            "Iteration 311, loss = 0.14919386\n",
            "Iteration 312, loss = 0.14917644\n",
            "Iteration 313, loss = 0.14916723\n",
            "Iteration 314, loss = 0.14913324\n",
            "Iteration 315, loss = 0.14907772\n",
            "Iteration 316, loss = 0.14905614\n",
            "Iteration 317, loss = 0.14910113\n",
            "Iteration 318, loss = 0.14913233\n",
            "Iteration 319, loss = 0.14906526\n",
            "Iteration 320, loss = 0.14905175\n",
            "Iteration 321, loss = 0.14900080\n",
            "Iteration 322, loss = 0.14903813\n",
            "Iteration 323, loss = 0.14893518\n",
            "Iteration 324, loss = 0.14894246\n",
            "Iteration 325, loss = 0.14889839\n",
            "Iteration 326, loss = 0.14891723\n",
            "Iteration 327, loss = 0.14888083\n",
            "Iteration 328, loss = 0.14882716\n",
            "Iteration 329, loss = 0.14891008\n",
            "Iteration 330, loss = 0.14885225\n",
            "Iteration 331, loss = 0.14878779\n",
            "Iteration 332, loss = 0.14881843\n",
            "Iteration 333, loss = 0.14870913\n",
            "Iteration 334, loss = 0.14882280\n",
            "Iteration 335, loss = 0.14880753\n",
            "Iteration 336, loss = 0.14870832\n",
            "Iteration 337, loss = 0.14862512\n",
            "Iteration 338, loss = 0.14866313\n",
            "Iteration 339, loss = 0.14857462\n",
            "Iteration 340, loss = 0.14859183\n",
            "Iteration 341, loss = 0.14861610\n",
            "Iteration 342, loss = 0.14858336\n",
            "Iteration 343, loss = 0.14854696\n",
            "Iteration 344, loss = 0.14858080\n",
            "Iteration 345, loss = 0.14852799\n",
            "Iteration 346, loss = 0.14859649\n",
            "Iteration 347, loss = 0.14853927\n",
            "Iteration 348, loss = 0.14853981\n",
            "Iteration 349, loss = 0.14850120\n",
            "Iteration 350, loss = 0.14842635\n",
            "Iteration 351, loss = 0.14843735\n",
            "Iteration 352, loss = 0.14841345\n",
            "Iteration 353, loss = 0.14839423\n",
            "Iteration 354, loss = 0.14828780\n",
            "Iteration 355, loss = 0.14834228\n",
            "Iteration 356, loss = 0.14832364\n",
            "Iteration 357, loss = 0.14827023\n",
            "Iteration 358, loss = 0.14822407\n",
            "Iteration 359, loss = 0.14822051\n",
            "Iteration 360, loss = 0.14823008\n",
            "Iteration 361, loss = 0.14817435\n",
            "Iteration 362, loss = 0.14815454\n",
            "Iteration 363, loss = 0.14813465\n",
            "Iteration 364, loss = 0.14815300\n",
            "Iteration 365, loss = 0.14814141\n",
            "Iteration 366, loss = 0.14808898\n",
            "Iteration 367, loss = 0.14815856\n",
            "Iteration 368, loss = 0.14810628\n",
            "Iteration 369, loss = 0.14808759\n",
            "Iteration 370, loss = 0.14799423\n",
            "Iteration 371, loss = 0.14803533\n",
            "Iteration 372, loss = 0.14799974\n",
            "Iteration 373, loss = 0.14796328\n",
            "Iteration 374, loss = 0.14796583\n",
            "Iteration 375, loss = 0.14796217\n",
            "Iteration 376, loss = 0.14790696\n",
            "Iteration 377, loss = 0.14792685\n",
            "Iteration 378, loss = 0.14786446\n",
            "Iteration 379, loss = 0.14780449\n",
            "Iteration 380, loss = 0.14784536\n",
            "Iteration 381, loss = 0.14780871\n",
            "Iteration 382, loss = 0.14779914\n",
            "Iteration 383, loss = 0.14774546\n",
            "Iteration 384, loss = 0.14767121\n",
            "Iteration 385, loss = 0.14771353\n",
            "Iteration 386, loss = 0.14770697\n",
            "Iteration 387, loss = 0.14760184\n",
            "Iteration 388, loss = 0.14769388\n",
            "Iteration 389, loss = 0.14767160\n",
            "Iteration 390, loss = 0.14766734\n",
            "Iteration 391, loss = 0.14756434\n",
            "Iteration 392, loss = 0.14760616\n",
            "Iteration 393, loss = 0.14758521\n",
            "Iteration 394, loss = 0.14755973\n",
            "Iteration 395, loss = 0.14747707\n",
            "Iteration 396, loss = 0.14749794\n",
            "Iteration 397, loss = 0.14746737\n",
            "Iteration 398, loss = 0.14742550\n",
            "Iteration 399, loss = 0.14736808\n",
            "Iteration 400, loss = 0.14736611\n",
            "Iteration 401, loss = 0.14738945\n",
            "Iteration 402, loss = 0.14732508\n",
            "Iteration 403, loss = 0.14738294\n",
            "Iteration 404, loss = 0.14736133\n",
            "Iteration 405, loss = 0.14731214\n",
            "Iteration 406, loss = 0.14722452\n",
            "Iteration 407, loss = 0.14721866\n",
            "Iteration 408, loss = 0.14717987\n",
            "Iteration 409, loss = 0.14715246\n",
            "Iteration 410, loss = 0.14714900\n",
            "Iteration 411, loss = 0.14708955\n",
            "Iteration 412, loss = 0.14706288\n",
            "Iteration 413, loss = 0.14711261\n",
            "Iteration 414, loss = 0.14701229\n",
            "Iteration 415, loss = 0.14704543\n",
            "Iteration 416, loss = 0.14700245\n",
            "Iteration 417, loss = 0.14705598\n",
            "Iteration 418, loss = 0.14700868\n",
            "Iteration 419, loss = 0.14702425\n",
            "Iteration 420, loss = 0.14701486\n",
            "Iteration 421, loss = 0.14690754\n",
            "Iteration 422, loss = 0.14685971\n",
            "Iteration 423, loss = 0.14688337\n",
            "Iteration 424, loss = 0.14686582\n",
            "Iteration 425, loss = 0.14677947\n",
            "Iteration 426, loss = 0.14674822\n",
            "Iteration 427, loss = 0.14679780\n",
            "Iteration 428, loss = 0.14675727\n",
            "Iteration 429, loss = 0.14665182\n",
            "Iteration 430, loss = 0.14674475\n",
            "Iteration 431, loss = 0.14666455\n",
            "Iteration 432, loss = 0.14654108\n",
            "Iteration 433, loss = 0.14657548\n",
            "Iteration 434, loss = 0.14668781\n",
            "Iteration 435, loss = 0.14663188\n",
            "Iteration 436, loss = 0.14659065\n",
            "Iteration 437, loss = 0.14661339\n",
            "Iteration 438, loss = 0.14648958\n",
            "Iteration 439, loss = 0.14641747\n",
            "Iteration 440, loss = 0.14640649\n",
            "Iteration 441, loss = 0.14647350\n",
            "Iteration 442, loss = 0.14635485\n",
            "Iteration 443, loss = 0.14633918\n",
            "Iteration 444, loss = 0.14637339\n",
            "Iteration 445, loss = 0.14638775\n",
            "Iteration 446, loss = 0.14634731\n",
            "Iteration 447, loss = 0.14630849\n",
            "Iteration 448, loss = 0.14621869\n",
            "Iteration 449, loss = 0.14629079\n",
            "Iteration 450, loss = 0.14627470\n",
            "Iteration 451, loss = 0.14618589\n",
            "Iteration 452, loss = 0.14624495\n",
            "Iteration 453, loss = 0.14618735\n",
            "Iteration 454, loss = 0.14606632\n",
            "Iteration 455, loss = 0.14617331\n",
            "Iteration 456, loss = 0.14605297\n",
            "Iteration 457, loss = 0.14601030\n",
            "Iteration 458, loss = 0.14607337\n",
            "Iteration 459, loss = 0.14604022\n",
            "Iteration 460, loss = 0.14600123\n",
            "Iteration 461, loss = 0.14600933\n",
            "Iteration 462, loss = 0.14604486\n",
            "Iteration 463, loss = 0.14591490\n",
            "Iteration 464, loss = 0.14588667\n",
            "Iteration 465, loss = 0.14594677\n",
            "Iteration 466, loss = 0.14592373\n",
            "Iteration 467, loss = 0.14583962\n",
            "Iteration 468, loss = 0.14594551\n",
            "Iteration 469, loss = 0.14577021\n",
            "Iteration 470, loss = 0.14583806\n",
            "Iteration 471, loss = 0.14569784\n",
            "Iteration 472, loss = 0.14570185\n",
            "Iteration 473, loss = 0.14574681\n",
            "Iteration 474, loss = 0.14568594\n",
            "Iteration 475, loss = 0.14565663\n",
            "Iteration 476, loss = 0.14554592\n",
            "Iteration 477, loss = 0.14560210\n",
            "Iteration 478, loss = 0.14560339\n",
            "Iteration 479, loss = 0.14558645\n",
            "Iteration 480, loss = 0.14546869\n",
            "Iteration 481, loss = 0.14554276\n",
            "Iteration 482, loss = 0.14547297\n",
            "Iteration 483, loss = 0.14544215\n",
            "Iteration 484, loss = 0.14540145\n",
            "Iteration 485, loss = 0.14545028\n",
            "Iteration 486, loss = 0.14545389\n",
            "Iteration 487, loss = 0.14530638\n",
            "Iteration 488, loss = 0.14543753\n",
            "Iteration 489, loss = 0.14529488\n",
            "Iteration 490, loss = 0.14535240\n",
            "Iteration 491, loss = 0.14521290\n",
            "Iteration 492, loss = 0.14523252\n",
            "Iteration 493, loss = 0.14530093\n",
            "Iteration 494, loss = 0.14525025\n",
            "Iteration 495, loss = 0.14518003\n",
            "Iteration 496, loss = 0.14506499\n",
            "Iteration 497, loss = 0.14514182\n",
            "Iteration 498, loss = 0.14501668\n",
            "Iteration 499, loss = 0.14509493\n",
            "Iteration 500, loss = 0.14509894\n",
            "Iteration 501, loss = 0.14510776\n",
            "Iteration 502, loss = 0.14504907\n",
            "Iteration 503, loss = 0.14498167\n",
            "Iteration 504, loss = 0.14498820\n",
            "Iteration 505, loss = 0.14503133\n",
            "Iteration 506, loss = 0.14487995\n",
            "Iteration 507, loss = 0.14494744\n",
            "Iteration 508, loss = 0.14495136\n",
            "Iteration 509, loss = 0.14480170\n",
            "Iteration 510, loss = 0.14482709\n",
            "Iteration 511, loss = 0.14480709\n",
            "Iteration 512, loss = 0.14471396\n",
            "Iteration 513, loss = 0.14481192\n",
            "Iteration 514, loss = 0.14479216\n",
            "Iteration 515, loss = 0.14468023\n",
            "Iteration 516, loss = 0.14466205\n",
            "Iteration 517, loss = 0.14462485\n",
            "Iteration 518, loss = 0.14452090\n",
            "Iteration 519, loss = 0.14457817\n",
            "Iteration 520, loss = 0.14458147\n",
            "Iteration 521, loss = 0.14465363\n",
            "Iteration 522, loss = 0.14449850\n",
            "Iteration 523, loss = 0.14453018\n",
            "Iteration 524, loss = 0.14441396\n",
            "Iteration 525, loss = 0.14451148\n",
            "Iteration 526, loss = 0.14446996\n",
            "Iteration 527, loss = 0.14454418\n",
            "Iteration 528, loss = 0.14443524\n",
            "Iteration 529, loss = 0.14441253\n",
            "Iteration 530, loss = 0.14426458\n",
            "Iteration 531, loss = 0.14434651\n",
            "Iteration 532, loss = 0.14440452\n",
            "Iteration 533, loss = 0.14426702\n",
            "Iteration 534, loss = 0.14410525\n",
            "Iteration 535, loss = 0.14432097\n",
            "Iteration 536, loss = 0.14415573\n",
            "Iteration 537, loss = 0.14422169\n",
            "Iteration 538, loss = 0.14414915\n",
            "Iteration 539, loss = 0.14415576\n",
            "Iteration 540, loss = 0.14404603\n",
            "Iteration 541, loss = 0.14396684\n",
            "Iteration 542, loss = 0.14408881\n",
            "Iteration 543, loss = 0.14401915\n",
            "Iteration 544, loss = 0.14400783\n",
            "Iteration 545, loss = 0.14391038\n",
            "Iteration 546, loss = 0.14391085\n",
            "Iteration 547, loss = 0.14388809\n",
            "Iteration 548, loss = 0.14393042\n",
            "Iteration 549, loss = 0.14395023\n",
            "Iteration 550, loss = 0.14387296\n",
            "Iteration 551, loss = 0.14380992\n",
            "Iteration 552, loss = 0.14378970\n",
            "Iteration 553, loss = 0.14386326\n",
            "Iteration 554, loss = 0.14376004\n",
            "Iteration 555, loss = 0.14365325\n",
            "Iteration 556, loss = 0.14373420\n",
            "Iteration 557, loss = 0.14371944\n",
            "Iteration 558, loss = 0.14367168\n",
            "Iteration 559, loss = 0.14351758\n",
            "Iteration 560, loss = 0.14357274\n",
            "Iteration 561, loss = 0.14355259\n",
            "Iteration 562, loss = 0.14350466\n",
            "Iteration 563, loss = 0.14360096\n",
            "Iteration 564, loss = 0.14345705\n",
            "Iteration 565, loss = 0.14343784\n",
            "Iteration 566, loss = 0.14338468\n",
            "Iteration 567, loss = 0.14341279\n",
            "Iteration 568, loss = 0.14335251\n",
            "Iteration 569, loss = 0.14337644\n",
            "Iteration 570, loss = 0.14341026\n",
            "Iteration 571, loss = 0.14335558\n",
            "Iteration 572, loss = 0.14329593\n",
            "Iteration 573, loss = 0.14324763\n",
            "Iteration 574, loss = 0.14317761\n",
            "Iteration 575, loss = 0.14326556\n",
            "Iteration 576, loss = 0.14319848\n",
            "Iteration 577, loss = 0.14315269\n",
            "Iteration 578, loss = 0.14312290\n",
            "Iteration 579, loss = 0.14322208\n",
            "Iteration 580, loss = 0.14313397\n",
            "Iteration 581, loss = 0.14299723\n",
            "Iteration 582, loss = 0.14317202\n",
            "Iteration 583, loss = 0.14302300\n",
            "Iteration 584, loss = 0.14312874\n",
            "Iteration 585, loss = 0.14296157\n",
            "Iteration 586, loss = 0.14297263\n",
            "Iteration 587, loss = 0.14295804\n",
            "Iteration 588, loss = 0.14295213\n",
            "Iteration 589, loss = 0.14280189\n",
            "Iteration 590, loss = 0.14281818\n",
            "Iteration 591, loss = 0.14281540\n",
            "Iteration 592, loss = 0.14290157\n",
            "Iteration 593, loss = 0.14275218\n",
            "Iteration 594, loss = 0.14267409\n",
            "Iteration 595, loss = 0.14270640\n",
            "Iteration 596, loss = 0.14273415\n",
            "Iteration 597, loss = 0.14266694\n",
            "Iteration 598, loss = 0.14257621\n",
            "Iteration 599, loss = 0.14264089\n",
            "Iteration 600, loss = 0.14256631\n",
            "Iteration 601, loss = 0.14251921\n",
            "Iteration 602, loss = 0.14251304\n",
            "Iteration 603, loss = 0.14249004\n",
            "Iteration 604, loss = 0.14252946\n",
            "Iteration 605, loss = 0.14243048\n",
            "Iteration 606, loss = 0.14247570\n",
            "Iteration 607, loss = 0.14237227\n",
            "Iteration 608, loss = 0.14226302\n",
            "Iteration 609, loss = 0.14242100\n",
            "Iteration 610, loss = 0.14225204\n",
            "Iteration 611, loss = 0.14230150\n",
            "Iteration 612, loss = 0.14219566\n",
            "Iteration 613, loss = 0.14230438\n",
            "Iteration 614, loss = 0.14227354\n",
            "Iteration 615, loss = 0.14229517\n",
            "Iteration 616, loss = 0.14226387\n",
            "Iteration 617, loss = 0.14206679\n",
            "Iteration 618, loss = 0.14207634\n",
            "Iteration 619, loss = 0.14213610\n",
            "Iteration 620, loss = 0.14208159\n",
            "Iteration 621, loss = 0.14203339\n",
            "Iteration 622, loss = 0.14202339\n",
            "Iteration 623, loss = 0.14196740\n",
            "Iteration 624, loss = 0.14186547\n",
            "Iteration 625, loss = 0.14197518\n",
            "Iteration 626, loss = 0.14188634\n",
            "Iteration 627, loss = 0.14193151\n",
            "Iteration 628, loss = 0.14190685\n",
            "Iteration 629, loss = 0.14197829\n",
            "Iteration 630, loss = 0.14186608\n",
            "Iteration 631, loss = 0.14184404\n",
            "Iteration 632, loss = 0.14178520\n",
            "Iteration 633, loss = 0.14178679\n",
            "Iteration 634, loss = 0.14176730\n",
            "Iteration 635, loss = 0.14165465\n",
            "Iteration 636, loss = 0.14160938\n",
            "Iteration 637, loss = 0.14165237\n",
            "Iteration 638, loss = 0.14161476\n",
            "Iteration 639, loss = 0.14160546\n",
            "Iteration 640, loss = 0.14163408\n",
            "Iteration 641, loss = 0.14157480\n",
            "Iteration 642, loss = 0.14161264\n",
            "Iteration 643, loss = 0.14142216\n",
            "Iteration 644, loss = 0.14156256\n",
            "Iteration 645, loss = 0.14138264\n",
            "Iteration 646, loss = 0.14138630\n",
            "Iteration 647, loss = 0.14131534\n",
            "Iteration 648, loss = 0.14146579\n",
            "Iteration 649, loss = 0.14142889\n",
            "Iteration 650, loss = 0.14130460\n",
            "Iteration 651, loss = 0.14129109\n",
            "Iteration 652, loss = 0.14134039\n",
            "Iteration 653, loss = 0.14117866\n",
            "Iteration 654, loss = 0.14134020\n",
            "Iteration 655, loss = 0.14115034\n",
            "Iteration 656, loss = 0.14118494\n",
            "Iteration 657, loss = 0.14111219\n",
            "Iteration 658, loss = 0.14118200\n",
            "Iteration 659, loss = 0.14112885\n",
            "Iteration 660, loss = 0.14112170\n",
            "Iteration 661, loss = 0.14112195\n",
            "Iteration 662, loss = 0.14108215\n",
            "Iteration 663, loss = 0.14104673\n",
            "Iteration 664, loss = 0.14107047\n",
            "Iteration 665, loss = 0.14091131\n",
            "Iteration 666, loss = 0.14097542\n",
            "Iteration 667, loss = 0.14093102\n",
            "Iteration 668, loss = 0.14082515\n",
            "Iteration 669, loss = 0.14089854\n",
            "Iteration 670, loss = 0.14079962\n",
            "Iteration 671, loss = 0.14082435\n",
            "Iteration 672, loss = 0.14071973\n",
            "Iteration 673, loss = 0.14077507\n",
            "Iteration 674, loss = 0.14090884\n",
            "Iteration 675, loss = 0.14071915\n",
            "Iteration 676, loss = 0.14071757\n",
            "Iteration 677, loss = 0.14067328\n",
            "Iteration 678, loss = 0.14053133\n",
            "Iteration 679, loss = 0.14067522\n",
            "Iteration 680, loss = 0.14059578\n",
            "Iteration 681, loss = 0.14061022\n",
            "Iteration 682, loss = 0.14065461\n",
            "Iteration 683, loss = 0.14057958\n",
            "Iteration 684, loss = 0.14058772\n",
            "Iteration 685, loss = 0.14048820\n",
            "Iteration 686, loss = 0.14048987\n",
            "Iteration 687, loss = 0.14036051\n",
            "Iteration 688, loss = 0.14047036\n",
            "Iteration 689, loss = 0.14038436\n",
            "Iteration 690, loss = 0.14028013\n",
            "Iteration 691, loss = 0.14019602\n",
            "Iteration 692, loss = 0.14031722\n",
            "Iteration 693, loss = 0.14035104\n",
            "Iteration 694, loss = 0.14032994\n",
            "Iteration 695, loss = 0.14039068\n",
            "Iteration 696, loss = 0.14016161\n",
            "Iteration 697, loss = 0.14014309\n",
            "Iteration 698, loss = 0.14023197\n",
            "Iteration 699, loss = 0.14006895\n",
            "Iteration 700, loss = 0.14003399\n",
            "Iteration 701, loss = 0.14006156\n",
            "Iteration 702, loss = 0.14011242\n",
            "Iteration 703, loss = 0.14002530\n",
            "Iteration 704, loss = 0.13996514\n",
            "Iteration 705, loss = 0.14009215\n",
            "Iteration 706, loss = 0.13995017\n",
            "Iteration 707, loss = 0.13982564\n",
            "Iteration 708, loss = 0.13985022\n",
            "Iteration 709, loss = 0.13982320\n",
            "Iteration 710, loss = 0.13983618\n",
            "Iteration 711, loss = 0.13993396\n",
            "Iteration 712, loss = 0.13978451\n",
            "Iteration 713, loss = 0.13972263\n",
            "Iteration 714, loss = 0.13955769\n",
            "Iteration 715, loss = 0.13970856\n",
            "Iteration 716, loss = 0.13965072\n",
            "Iteration 717, loss = 0.13981831\n",
            "Iteration 718, loss = 0.13968422\n",
            "Iteration 719, loss = 0.13971952\n",
            "Iteration 720, loss = 0.13972276\n",
            "Iteration 721, loss = 0.13961536\n",
            "Iteration 722, loss = 0.13952875\n",
            "Iteration 723, loss = 0.13948649\n",
            "Iteration 724, loss = 0.13943208\n",
            "Iteration 725, loss = 0.13944918\n",
            "Iteration 726, loss = 0.13955395\n",
            "Iteration 727, loss = 0.13945940\n",
            "Iteration 728, loss = 0.13933544\n",
            "Iteration 729, loss = 0.13943552\n",
            "Iteration 730, loss = 0.13935692\n",
            "Iteration 731, loss = 0.13929696\n",
            "Iteration 732, loss = 0.13926666\n",
            "Iteration 733, loss = 0.13923120\n",
            "Iteration 734, loss = 0.13935404\n",
            "Iteration 735, loss = 0.13930240\n",
            "Iteration 736, loss = 0.13936688\n",
            "Iteration 737, loss = 0.13910558\n",
            "Iteration 738, loss = 0.13919721\n",
            "Iteration 739, loss = 0.13918898\n",
            "Iteration 740, loss = 0.13903018\n",
            "Iteration 741, loss = 0.13913806\n",
            "Iteration 742, loss = 0.13909040\n",
            "Iteration 743, loss = 0.13910382\n",
            "Iteration 744, loss = 0.13901259\n",
            "Iteration 745, loss = 0.13902961\n",
            "Iteration 746, loss = 0.13893749\n",
            "Iteration 747, loss = 0.13897904\n",
            "Iteration 748, loss = 0.13892951\n",
            "Iteration 749, loss = 0.13886557\n",
            "Iteration 750, loss = 0.13887461\n",
            "Iteration 751, loss = 0.13886497\n",
            "Iteration 752, loss = 0.13878098\n",
            "Iteration 753, loss = 0.13885720\n",
            "Iteration 754, loss = 0.13868086\n",
            "Iteration 755, loss = 0.13873672\n",
            "Iteration 756, loss = 0.13871219\n",
            "Iteration 757, loss = 0.13877063\n",
            "Iteration 758, loss = 0.13875321\n",
            "Iteration 759, loss = 0.13854770\n",
            "Iteration 760, loss = 0.13866178\n",
            "Iteration 761, loss = 0.13864132\n",
            "Iteration 762, loss = 0.13867294\n",
            "Iteration 763, loss = 0.13846224\n",
            "Iteration 764, loss = 0.13849405\n",
            "Iteration 765, loss = 0.13844001\n",
            "Iteration 766, loss = 0.13849558\n",
            "Iteration 767, loss = 0.13840526\n",
            "Iteration 768, loss = 0.13853043\n",
            "Iteration 769, loss = 0.13850943\n",
            "Iteration 770, loss = 0.13845683\n",
            "Iteration 771, loss = 0.13833142\n",
            "Iteration 772, loss = 0.13821799\n",
            "Iteration 773, loss = 0.13833331\n",
            "Iteration 774, loss = 0.13834865\n",
            "Iteration 775, loss = 0.13819779\n",
            "Iteration 776, loss = 0.13814436\n",
            "Iteration 777, loss = 0.13816861\n",
            "Iteration 778, loss = 0.13808257\n",
            "Iteration 779, loss = 0.13805734\n",
            "Iteration 780, loss = 0.13805228\n",
            "Iteration 781, loss = 0.13821358\n",
            "Iteration 782, loss = 0.13806470\n",
            "Iteration 783, loss = 0.13811975\n",
            "Iteration 784, loss = 0.13807493\n",
            "Iteration 785, loss = 0.13806188\n",
            "Iteration 786, loss = 0.13796940\n",
            "Iteration 787, loss = 0.13793246\n",
            "Iteration 788, loss = 0.13799422\n",
            "Iteration 789, loss = 0.13788231\n",
            "Iteration 790, loss = 0.13779485\n",
            "Iteration 791, loss = 0.13784417\n",
            "Iteration 792, loss = 0.13764202\n",
            "Iteration 793, loss = 0.13769823\n",
            "Iteration 794, loss = 0.13777363\n",
            "Iteration 795, loss = 0.13785317\n",
            "Iteration 796, loss = 0.13761185\n",
            "Iteration 797, loss = 0.13762332\n",
            "Iteration 798, loss = 0.13766168\n",
            "Iteration 799, loss = 0.13770553\n",
            "Iteration 800, loss = 0.13766624\n",
            "Iteration 801, loss = 0.13764401\n",
            "Iteration 802, loss = 0.13762450\n",
            "Iteration 803, loss = 0.13752712\n",
            "Iteration 804, loss = 0.13753875\n",
            "Iteration 805, loss = 0.13748262\n",
            "Iteration 806, loss = 0.13733572\n",
            "Iteration 807, loss = 0.13747105\n",
            "Iteration 808, loss = 0.13729047\n",
            "Iteration 809, loss = 0.13750596\n",
            "Iteration 810, loss = 0.13743384\n",
            "Iteration 811, loss = 0.13728168\n",
            "Iteration 812, loss = 0.13740069\n",
            "Iteration 813, loss = 0.13729753\n",
            "Iteration 814, loss = 0.13728314\n",
            "Iteration 815, loss = 0.13718330\n",
            "Iteration 816, loss = 0.13724101\n",
            "Iteration 817, loss = 0.13723064\n",
            "Iteration 818, loss = 0.13731638\n",
            "Iteration 819, loss = 0.13715201\n",
            "Iteration 820, loss = 0.13710233\n",
            "Iteration 821, loss = 0.13716482\n",
            "Iteration 822, loss = 0.13717732\n",
            "Iteration 823, loss = 0.13697293\n",
            "Iteration 824, loss = 0.13695549\n",
            "Iteration 825, loss = 0.13700452\n",
            "Iteration 826, loss = 0.13690548\n",
            "Iteration 827, loss = 0.13686951\n",
            "Iteration 828, loss = 0.13694921\n",
            "Iteration 829, loss = 0.13699244\n",
            "Iteration 830, loss = 0.13691500\n",
            "Iteration 831, loss = 0.13676266\n",
            "Iteration 832, loss = 0.13691922\n",
            "Iteration 833, loss = 0.13666712\n",
            "Iteration 834, loss = 0.13665736\n",
            "Iteration 835, loss = 0.13669092\n",
            "Iteration 836, loss = 0.13686072\n",
            "Iteration 837, loss = 0.13659845\n",
            "Iteration 838, loss = 0.13677182\n",
            "Iteration 839, loss = 0.13663799\n",
            "Iteration 840, loss = 0.13669291\n",
            "Iteration 841, loss = 0.13659412\n",
            "Iteration 842, loss = 0.13666661\n",
            "Iteration 843, loss = 0.13651712\n",
            "Iteration 844, loss = 0.13651484\n",
            "Iteration 845, loss = 0.13646782\n",
            "Iteration 846, loss = 0.13654306\n",
            "Iteration 847, loss = 0.13651573\n",
            "Iteration 848, loss = 0.13643522\n",
            "Iteration 849, loss = 0.13644256\n",
            "Iteration 850, loss = 0.13637307\n",
            "Iteration 851, loss = 0.13646250\n",
            "Iteration 852, loss = 0.13640634\n",
            "Iteration 853, loss = 0.13628858\n",
            "Iteration 854, loss = 0.13627838\n",
            "Iteration 855, loss = 0.13620753\n",
            "Iteration 856, loss = 0.13622976\n",
            "Iteration 857, loss = 0.13630035\n",
            "Iteration 858, loss = 0.13621605\n",
            "Iteration 859, loss = 0.13627648\n",
            "Iteration 860, loss = 0.13622199\n",
            "Iteration 861, loss = 0.13621503\n",
            "Iteration 862, loss = 0.13618426\n",
            "Iteration 863, loss = 0.13605872\n",
            "Iteration 864, loss = 0.13603172\n",
            "Iteration 865, loss = 0.13599551\n",
            "Iteration 866, loss = 0.13604192\n",
            "Iteration 867, loss = 0.13599866\n",
            "Iteration 868, loss = 0.13597962\n",
            "Iteration 869, loss = 0.13598455\n",
            "Iteration 870, loss = 0.13584039\n",
            "Iteration 871, loss = 0.13572418\n",
            "Iteration 872, loss = 0.13585698\n",
            "Iteration 873, loss = 0.13585204\n",
            "Iteration 874, loss = 0.13586791\n",
            "Iteration 875, loss = 0.13586940\n",
            "Iteration 876, loss = 0.13579083\n",
            "Iteration 877, loss = 0.13562422\n",
            "Iteration 878, loss = 0.13571434\n",
            "Iteration 879, loss = 0.13575437\n",
            "Iteration 880, loss = 0.13562467\n",
            "Iteration 881, loss = 0.13554957\n",
            "Iteration 882, loss = 0.13564121\n",
            "Iteration 883, loss = 0.13563336\n",
            "Iteration 884, loss = 0.13554871\n",
            "Iteration 885, loss = 0.13549224\n",
            "Iteration 886, loss = 0.13550442\n",
            "Iteration 887, loss = 0.13567210\n",
            "Iteration 888, loss = 0.13541158\n",
            "Iteration 889, loss = 0.13534122\n",
            "Iteration 890, loss = 0.13555199\n",
            "Iteration 891, loss = 0.13548114\n",
            "Iteration 892, loss = 0.13546085\n",
            "Iteration 893, loss = 0.13547275\n",
            "Iteration 894, loss = 0.13533579\n",
            "Iteration 895, loss = 0.13541283\n",
            "Iteration 896, loss = 0.13531105\n",
            "Iteration 897, loss = 0.13529360\n",
            "Iteration 898, loss = 0.13528144\n",
            "Iteration 899, loss = 0.13518649\n",
            "Iteration 900, loss = 0.13526268\n",
            "Iteration 901, loss = 0.13525624\n",
            "Iteration 902, loss = 0.13501630\n",
            "Iteration 903, loss = 0.13516184\n",
            "Iteration 904, loss = 0.13516203\n",
            "Iteration 905, loss = 0.13512611\n",
            "Iteration 906, loss = 0.13510482\n",
            "Iteration 907, loss = 0.13494148\n",
            "Iteration 908, loss = 0.13494400\n",
            "Iteration 909, loss = 0.13502468\n",
            "Iteration 910, loss = 0.13509971\n",
            "Iteration 911, loss = 0.13484016\n",
            "Iteration 912, loss = 0.13498385\n",
            "Iteration 913, loss = 0.13483239\n",
            "Iteration 914, loss = 0.13488143\n",
            "Iteration 915, loss = 0.13489293\n",
            "Iteration 916, loss = 0.13481887\n",
            "Iteration 917, loss = 0.13474709\n",
            "Iteration 918, loss = 0.13473006\n",
            "Iteration 919, loss = 0.13477724\n",
            "Iteration 920, loss = 0.13477188\n",
            "Iteration 921, loss = 0.13457365\n",
            "Iteration 922, loss = 0.13469116\n",
            "Iteration 923, loss = 0.13460287\n",
            "Iteration 924, loss = 0.13467396\n",
            "Iteration 925, loss = 0.13459659\n",
            "Iteration 926, loss = 0.13461946\n",
            "Iteration 927, loss = 0.13463457\n",
            "Iteration 928, loss = 0.13456266\n",
            "Iteration 929, loss = 0.13449594\n",
            "Iteration 930, loss = 0.13455967\n",
            "Iteration 931, loss = 0.13451247\n",
            "Iteration 932, loss = 0.13441297\n",
            "Iteration 933, loss = 0.13432447\n",
            "Iteration 934, loss = 0.13441351\n",
            "Iteration 935, loss = 0.13422484\n",
            "Iteration 936, loss = 0.13457342\n",
            "Iteration 937, loss = 0.13431014\n",
            "Iteration 938, loss = 0.13407145\n",
            "Iteration 939, loss = 0.13418894\n",
            "Iteration 940, loss = 0.13428807\n",
            "Iteration 941, loss = 0.13416407\n",
            "Iteration 942, loss = 0.13426120\n",
            "Iteration 943, loss = 0.13411341\n",
            "Iteration 944, loss = 0.13422073\n",
            "Iteration 945, loss = 0.13419050\n",
            "Iteration 946, loss = 0.13414499\n",
            "Iteration 947, loss = 0.13418422\n",
            "Iteration 948, loss = 0.13424344\n",
            "Iteration 949, loss = 0.13412797\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
              "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "              hidden_layer_sizes=(100, 100, 100), learning_rate='constant',\n",
              "              learning_rate_init=0.001, max_fun=15000, max_iter=1000,\n",
              "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
              "              power_t=0.5, random_state=21, shuffle=True, solver='sgd',\n",
              "              tol=1e-09, validation_fraction=0.1, verbose=10, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GI29V269cDHb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = clf.predict(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2won9rBmcQ1D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9eee4576-4f9f-4b57-f4a1-261dc6c5b76e"
      },
      "source": [
        "accuracy_score(y_test,y_pred)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9624505430810716"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ay67AZSicc7y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "7edc420a-50ed-4ba9-f04a-7b524bf45678"
      },
      "source": [
        "confusion_matrix(y_test, y_pred)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[114564,    130],\n",
              "       [  4340,      9]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHeH0qmZchOn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "cbe304e6-5fb6-4fca-c3b7-e6cefb0746f4"
      },
      "source": [
        "print(classification_report(y_test,y_pred))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      1.00      0.98    114694\n",
            "           1       0.06      0.00      0.00      4349\n",
            "\n",
            "    accuracy                           0.96    119043\n",
            "   macro avg       0.51      0.50      0.49    119043\n",
            "weighted avg       0.93      0.96      0.95    119043\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LynE6Ncjc1y8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "fe3cee1b-2610-433a-d2b3-06a99425578c"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.heatmap(cm, center=True)\n",
        "plt.show()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXMAAAD4CAYAAAAeugY9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nr6QAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}